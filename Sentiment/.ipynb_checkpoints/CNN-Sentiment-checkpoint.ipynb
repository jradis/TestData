{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.datasets import imdb\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam, Adamax\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>@jamielewislewis i cant believe it, it really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>having a vodka tonic and looking forward to go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>@ddlovatofans1neg1 Could you follow me please....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>@jordanknight for once.................. PLEAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>Had a dream about a walk in fast food resturau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   neg  @jamielewislewis i cant believe it, it really ...\n",
       "1   pos  having a vodka tonic and looking forward to go...\n",
       "2   pos  @ddlovatofans1neg1 Could you follow me please....\n",
       "3   pos  @jordanknight for once.................. PLEAS...\n",
       "4   neg  Had a dream about a walk in fast food resturau..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('sentiment.tsv', delimiter='\\t', header=None, names=['label', 'text'])\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare the data to evaluate\n",
    "score = pd.get_dummies(ds['label'], drop_first=True).values\n",
    "Y = np.ravel(score)\n",
    "texts = ds['text'].values\n",
    "#Convert twitter call-signs to the same word\n",
    "#I was going to use 'fish' because it is singular and plural\n",
    "#decided to use person because I thought it would match the embedding vectors better\n",
    "CleanText = []\n",
    "for z in texts:\n",
    "    T = re.sub(r'[^@\\s]*@\\S*', 'person', z)\n",
    "    CleanText.append(T)\n",
    "CleanText = np.asarray(CleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set Params\n",
    "MAX_NB_WORDS = 6105\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 50\n",
    "IMBD_TRAINING_DIR = \"./test\"\n",
    "IMBD_TESTING_DIR = \"./train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_samples(TEXT_DATA_DIR):\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []\n",
    "    for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "        path = os.path.join(TEXT_DATA_DIR, name)\n",
    "        if os.path.isdir(path):\n",
    "            label_id = len(labels_index)\n",
    "            labels_index[name] = label_id\n",
    "            for fname in sorted(os.listdir(path)):\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "       \n",
    "    return np.asarray(texts), np.asarray(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ximbd_test, Yimbd_test = get_text_samples(IMBD_TESTING_DIR)\n",
    "Ximbd_train, Yimbd_train = get_text_samples(IMBD_TRAINING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a word_index for all words in both datasets\n",
    "ALLTEXTS = np.concatenate((Ximbd_test, Ximbd_train, CleanText))\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(ALLTEXTS)\n",
    "\n",
    "#Create numerical representation of text\n",
    "def get_sequences(texts, tokenizer):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    X = pad_sequences(sequences, maxlen=500)\n",
    "    return X\n",
    "XTwitter = get_sequences(CleanText, tokenizer)\n",
    "Ximbd_test = get_sequences(Ximbd_test, tokenizer)\n",
    "Ximbd_train = get_sequences(Ximbd_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2001, 500)\n",
      "Shape of label tensor: (2001,)\n",
      "Shape of IMBD tensor: (25000, 500)\n",
      "Shape of IMBD label tensor: (25000,)\n",
      "Max 125257\n",
      "Max 125285\n",
      "Max 125289\n"
     ]
    }
   ],
   "source": [
    "#Check to make sure that your tensors are shaped properly\n",
    "print 'Shape of data tensor:', XTwitter.shape\n",
    "print 'Shape of label tensor:', Y.shape\n",
    "print 'Shape of IMBD tensor:', Ximbd_test.shape\n",
    "print 'Shape of IMBD label tensor:', Yimbd_test.shape\n",
    "\n",
    "#Get the embedding size\n",
    "print 'Max', XTwitter.max()\n",
    "print 'Max', Ximbd_test.max()\n",
    "print 'Max', Ximbd_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 50)           6264500   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 500, 32)           4832      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 8,269,833\n",
      "Trainable params: 8,269,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 112s - loss: 0.3508 - acc: 0.8213 - val_loss: 0.3038 - val_acc: 0.8774\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 97s - loss: 0.1111 - acc: 0.9610 - val_loss: 0.3366 - val_acc: 0.8744\n",
      "Accuracy: 87.44%\n"
     ]
    }
   ],
   "source": [
    "## create the BaseModel\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(125290, 32, input_length=500))\n",
    "model.add(Conv1D(64, 3, padding='same'))\n",
    "model.add(Conv1D(32, 3, padding='same'))\n",
    "model.add(Conv1D(16, 3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(125290, 50, input_length=500))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(125290, 100, input_length=500))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "print model.summary()\n",
    "# Fit the BaseModel\n",
    "model.fit(Ximbd_train, Yimbd_train, validation_data=(Ximbd_test, Yimbd_test), epochs=2, batch_size=64, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(Ximbd_test, Yimbd_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#save the model since you can't deep copy a NN while using Keras+Tensorflow\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "print\"Saved model to disk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Training Fold 1 of 10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 66s - loss: 0.6290 - acc: 0.6528    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: assignment will raise an error in the future, most likely because your index result shape does not match the value array shape. You can use `arr.flat[index] = values` to keep the old behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Training Fold 2 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 66s - loss: 0.6410 - acc: 0.6458    \n",
      "Loaded model from disk\n",
      "Training Fold 3 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 61s - loss: 0.6262 - acc: 0.6446    \n",
      "Loaded model from disk\n",
      "Training Fold 4 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 64s - loss: 0.6264 - acc: 0.6513    \n",
      "Loaded model from disk\n",
      "Training Fold 5 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 64s - loss: 0.6386 - acc: 0.6285    \n",
      "Loaded model from disk\n",
      "Training Fold 6 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 68s - loss: 0.6248 - acc: 0.6574    \n",
      "Loaded model from disk\n",
      "Training Fold 7 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 65s - loss: 0.6277 - acc: 0.6524    \n",
      "Loaded model from disk\n",
      "Training Fold 8 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 64s - loss: 0.6450 - acc: 0.6446    \n",
      "Loaded model from disk\n",
      "Training Fold 9 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 66s - loss: 0.6404 - acc: 0.6308    \n",
      "Loaded model from disk\n",
      "Training Fold 10 of 10\n",
      "Epoch 1/1\n",
      "1801/1801 [==============================] - 65s - loss: 0.6363 - acc: 0.6496    \n",
      "0.752469123466\n"
     ]
    }
   ],
   "source": [
    "#Run 1 epoch on each fold. Store Predictions. Evaluate AUC ROC\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "testPreds = np.zeros_like(Y, dtype=float)\n",
    "counter = 1\n",
    "for train_set, test_set in kf.split(XTwitter, Y):\n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"model.h5\")\n",
    "    print \"Loaded model from disk\"\n",
    "    print \"Training Fold %s of 10\" %str(counter)\n",
    "    ADAM = Adamax(lr=.005)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=ADAM, metrics=['accuracy'])\n",
    "    #run fit model to fold\n",
    "    model.fit(XTwitter[train_set], Y[train_set], verbose=1, epochs=1, batch_size=4)\n",
    "    testPreds[test_set] = model.predict(XTwitter[test_set])\n",
    "    counter += 1\n",
    "print roc_auc_score(Y, testPreds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If you would rather use a different embedding you could use the wiki-set\n",
    "'''\n",
    "#Use large dataset embeddings\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
